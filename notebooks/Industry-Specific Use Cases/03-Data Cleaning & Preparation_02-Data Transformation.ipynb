{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e3419cc",
   "metadata": {},
   "source": [
    "The next topic in the data preparation series is **Data Cleaning & Preparation - 2. Data Transformation**.\n",
    "\n",
    "Data transformation is the process of modifying data to address issues identified during quality assessment and prepare it for analysis or modeling. This essential phase includes techniques for handling missing values, correcting data types, creating new features, scaling data, and encoding categorical variables.\n",
    "\n",
    "Based on [`piplist.txt`](./README.md) output, we have robust libraries for this, including `pandas` (for core manipulation), `numpy`, and importantly, `scikit-learn` which provides excellent tools for preprocessing like imputation, scaling, and encoding.\n",
    "\n",
    "**Step 1: Generate Sample Data for Transformation**\n",
    "\n",
    "We'll generate data similar to the previous step but ensure it includes specific scenarios that require common transformations, like numerical columns with NaNs for imputation, categorical columns for encoding, and numerical columns for scaling.\n",
    "\n",
    "In a new Excel cell, enter `=PY` and paste the following code, then press **Ctrl+Enter**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a6251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy data requiring various transformations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import date, timedelta\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "num_records = 800\n",
    "\n",
    "data = {\n",
    "    'RecordID': range(1, num_records + 1),\n",
    "    'CustomerName': [fake.name() for _ in range(num_records)],\n",
    "    'PurchaseAmount': [round(random.uniform(10, 500), 2) if random.random() > 0.05 else np.nan for _ in range(num_records)], # Missing amounts\n",
    "    'Rating': [random.choice([1, 2, 3, 4, 5, None]) for _ in range(num_records)], # Missing ratings\n",
    "    'ProductCategory': [random.choice(['Electronics', 'Clothing', 'Home Goods', 'Groceries', 'Books', 'Other']) for _ in range(num_records)],\n",
    "    'Region': [random.choice(['North', 'South', 'East', 'West', 'Central', 'Central']) for _ in range(num_records)], # Some imbalance\n",
    "    'IsLoyaltyMember': [random.choice([True, False, None]) for _ in range(num_records)], # Boolean with missing\n",
    "    'JoinDate': [fake.date_between(start_date='-5y', end_date='today') if random.random() > 0.03 else None for _ in range(num_records)], # Dates with missing\n",
    "    'FeedbackText': [fake.sentence() if random.random() > 0.1 else '' for _ in range(num_records)], # Text, some empty\n",
    "    'NumberOfVisits': [random.randint(1, 50) if random.random() > 0.04 else np.nan for _ in range(num_records)] # Missing visits\n",
    "}\n",
    "\n",
    "df_transform_raw = pd.DataFrame(data)\n",
    "\n",
    "# Add some duplicate records\n",
    "duplicate_records = df_transform_raw.sample(n=int(num_records * 0.03), replace=False)\n",
    "df_transform_raw = pd.concat([df_transform_raw, duplicate_records], ignore_index=True)\n",
    "\n",
    "# Add an outlier\n",
    "df_transform_raw.loc[random.randint(0, len(df_transform_raw)-1), 'PurchaseAmount'] = 50000\n",
    "\n",
    "\n",
    "# Shuffle for good measure\n",
    "df_transform_raw = df_transform_raw.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df_transform_raw # Output the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29047e3d",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "*   This code generates a DataFrame `df_transform_raw` with columns representing customer-related data.\n",
    "*   It includes columns with:\n",
    "    *   Missing numerical values (`PurchaseAmount`, `Rating`, `NumberOfVisits`).\n",
    "    *   Categorical text (`ProductCategory`, `Region`).\n",
    "    *   Boolean values (`IsLoyaltyMember`) with missing entries.\n",
    "    *   Date values (`JoinDate`) with missing entries.\n",
    "    *   Text data (`FeedbackText`).\n",
    "    *   Duplicate rows.\n",
    "    *   An artificial outlier in `PurchaseAmount`.\n",
    "*   The result, `df_transform_raw`, will be spilled into your Excel sheet. Let's assume this data is placed in a range or Table named `DataForTransformation`.\n",
    "\n",
    "**Step 2: Perform Data Transformations**\n",
    "\n",
    "Now, we'll apply various transformations using `pandas` and `scikit-learn`.\n",
    "\n",
    "In a **new** Excel cell, enter `=PY` and paste the following code. Replace `\"DataForTransformation\"` with the actual name of the Excel range/Table where your dummy data is. Press **Ctrl+Enter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b5e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform various data transformations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer # For handling missing values\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler # For encoding and scaling\n",
    "from sklearn.compose import ColumnTransformer # To apply different transformers to different columns\n",
    "from sklearn.pipeline import Pipeline # To chain transformations\n",
    "from datetime import date, timedelta # Import timedelta\n",
    "\n",
    "# Load the raw data from Excel\n",
    "# IMPORTANT: Replace \"DataForTransformation\" with the actual name of your Excel range or Table\n",
    "df_raw = xl(\"DataForTransformation[#All]\", headers=True)\n",
    "# Create a copy to work on, preserving the original raw data\n",
    "df_transformed = df_raw.copy()\n",
    "\n",
    "\n",
    "# --- Transformation 1: Handle Duplicate Rows ---\n",
    "initial_rows = len(df_transformed)\n",
    "df_transformed.drop_duplicates(inplace=True)\n",
    "rows_after_dropping_duplicates = len(df_transformed)\n",
    "duplicate_rows_removed = initial_rows - rows_after_dropping_duplicates\n",
    "\n",
    "\n",
    "# --- Transformation 2: Handle Missing Values (Imputation) ---\n",
    "# Identify columns for imputation\n",
    "# Numeric columns: Impute with median (robust to outliers)\n",
    "numeric_cols_for_imputation = ['PurchaseAmount', 'Rating', 'NumberOfVisits']\n",
    "# Categorical column (Boolean): Impute with most frequent value\n",
    "categorical_cols_for_imputation = ['IsLoyaltyMember'] # Note: Nones in bool might be read as object type\n",
    "\n",
    "# Use Scikit-learn's SimpleImputer for a structured approach\n",
    "# Create transformers for different column types\n",
    "numeric_transformer = SimpleImputer(strategy='median')\n",
    "categorical_transformer = SimpleImputer(strategy='most_frequent') # Works for object/string types too\n",
    "\n",
    "# Define the columns to apply these transformations to\n",
    "# Need to handle potential type issues from Excel import, especially for 'IsLoyaltyMember'\n",
    "# Ensure 'IsLoyaltyMember' is treated as object/string before imputation\n",
    "df_transformed['IsLoyaltyMember'] = df_transformed['IsLoyaltyMember'].astype(object) # Treat as generic object type for Imputer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols_for_imputation),\n",
    "        ('cat', categorical_transformer, categorical_cols_for_imputation)\n",
    "    ],\n",
    "    remainder='passthrough' # Keep other columns (like RecordID, CustomerName, ProductCategory, Region, JoinDate, FeedbackText)\n",
    ")\n",
    "\n",
    "# Apply the imputation\n",
    "# The output of ColumnTransformer is a numpy array, remember original column order/names\n",
    "# Need to reconstruct DataFrame carefully, especially with 'remainder'\n",
    "transformed_data_array = preprocessor.fit_transform(df_transformed)\n",
    "\n",
    "# Get names of columns passed through + transformed columns\n",
    "# preprocessor.get_feature_names_out() works in newer versions, fallback needed for older\n",
    "# Manual tracking: original columns - imputed columns + imputed columns\n",
    "remaining_cols = [col for col in df_transformed.columns if col not in numeric_cols_for_imputation + categorical_cols_for_imputation]\n",
    "imputed_cols_order = numeric_cols_for_imputation + categorical_cols_for_imputation\n",
    "\n",
    "# Reconstruct DataFrame\n",
    "df_transformed_imputed = pd.DataFrame(transformed_data_array, columns=imputed_cols_order + remaining_cols)\n",
    "\n",
    "# Ensure columns are in a logical order and types are correct after transformation\n",
    "# ColumnTransformer might change column order and dtypes. Let's re-map and cast.\n",
    "# Identify columns before and after transformation based on preprocessor\n",
    "transformed_feature_names = numeric_cols_for_imputation + categorical_cols_for_imputation + remaining_cols # Assuming this is the order\n",
    "\n",
    "# Reconstruct DataFrame with correct column names and attempt type casting\n",
    "df_transformed_imputed = pd.DataFrame(transformed_data_array, columns=transformed_feature_names)\n",
    "\n",
    "# Cast columns back to appropriate types after imputation and potential numpy conversion\n",
    "for col in numeric_cols_for_imputation:\n",
    "     df_transformed_imputed[col] = pd.to_numeric(df_transformed_imputed[col], errors='coerce') # Should be numeric after median imputation\n",
    "df_transformed_imputed['IsLoyaltyMember'] = df_transformed_imputed['IsLoyaltyMember'].astype(bool) # Should be bool after most frequent imputation (assuming True/False were present)\n",
    "# Re-convert Date column which might be object/string now\n",
    "df_transformed_imputed['JoinDate'] = pd.to_datetime(df_transformed_imputed['JoinDate'], errors='coerce')\n",
    "\n",
    "\n",
    "# --- Transformation 3: Correct/Convert Data Types ---\n",
    "# Ensure numerical columns are numeric (done during imputation reconstruction, but double check)\n",
    "# df_transformed_imputed['PurchaseAmount'] = pd.to_numeric(df_transformed_imputed['PurchaseAmount'], errors='coerce') # Already handled above\n",
    "# df_transformed_imputed['Rating'] = pd.to_numeric(df_transformed_imputed['Rating'], errors='coerce') # Already handled above\n",
    "# df_transformed_imputed['NumberOfVisits'] = pd.to_numeric(df_transformed_imputed['NumberOfVisits'], errors='coerce') # Already handled above\n",
    "# Ensure Boolean column is boolean (handled above)\n",
    "# Ensure Date column is datetime (handled above)\n",
    "# Ensure Text column is string (usually read correctly, but can force)\n",
    "df_transformed_imputed['FeedbackText'] = df_transformed_imputed['FeedbackText'].astype(str)\n",
    "\n",
    "\n",
    "# --- Transformation 4: Feature Engineering (Example: Days Since Join) ---\n",
    "# Calculate days since join date (relative to today, or a fixed date)\n",
    "# Use a fixed analysis date for reproducibility\n",
    "analysis_date = pd.to_datetime(date(2024, 6, 1)) # Convert analysis_date to datetime\n",
    "df_transformed_imputed['DaysSinceJoin'] = (analysis_date - df_transformed_imputed['JoinDate']).dt.days # Use the .dt accessor after ensuring datetime type\n",
    "\n",
    "# Handle cases where JoinDate was originally missing and is NaT (Not a Time)\n",
    "df_transformed_imputed['DaysSinceJoin'] = df_transformed_imputed['DaysSinceJoin'].fillna(-1) # Use a sentinel value like -1\n",
    "\n",
    "\n",
    "# --- Transformation 5: Handle Categorical Data (One-Hot Encoding) ---\n",
    "# Select columns for one-hot encoding\n",
    "categorical_cols_for_encoding = ['ProductCategory', 'Region']\n",
    "\n",
    "# Use Scikit-learn's OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False) # handle_unknown='ignore' for unseen categories, sparse=False for dense array\n",
    "\n",
    "# Fit and transform the selected columns\n",
    "encoded_data = encoder.fit_transform(df_transformed_imputed[categorical_cols_for_encoding])\n",
    "\n",
    "# Create a DataFrame from the encoded data with meaningful column names\n",
    "encoded_col_names = encoder.get_feature_names_out(categorical_cols_for_encoding)\n",
    "df_encoded = pd.DataFrame(encoded_data, columns=encoded_col_names, index=df_transformed_imputed.index)\n",
    "\n",
    "# Drop the original categorical columns and join the new encoded columns\n",
    "df_transformed_encoded = df_transformed_imputed.drop(columns=categorical_cols_for_encoding)\n",
    "df_transformed_final = df_transformed_encoded.join(df_encoded)\n",
    "\n",
    "\n",
    "# --- Transformation 6: Standardize Numerical Data ---\n",
    "# Select numerical columns for standardization (exclude ID, engineered features if not needed)\n",
    "# Exclude engineered 'DaysSinceJoin' for this example, or include it if needed\n",
    "numerical_cols_for_scaling = ['PurchaseAmount', 'Rating', 'NumberOfVisits'] # Use the imputed columns\n",
    "\n",
    "# Use Scikit-learn's StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the selected numerical columns\n",
    "# Need to handle potential NaNs if imputation wasn't perfect, but SimpleImputer should have handled them\n",
    "df_transformed_final[numerical_cols_for_scaling] = scaler.fit_transform(df_transformed_final[numerical_cols_for_scaling])\n",
    "\n",
    "\n",
    "# --- Summarize Transformations ---\n",
    "transformation_summary = pd.DataFrame({\n",
    "    'Transformation': [\n",
    "        'Duplicate Rows Removed',\n",
    "        'Missing Values Imputed (Numeric: Median)',\n",
    "        'Missing Values Imputed (Categorical: Most Frequent)',\n",
    "        'Data Types Corrected',\n",
    "        'Feature Engineered (Days Since Join)',\n",
    "        'Categorical Columns One-Hot Encoded',\n",
    "        'Numerical Columns Standardized'\n",
    "    ],\n",
    "    'Details': [\n",
    "        duplicate_rows_removed,\n",
    "        f\"Columns: {', '.join(numeric_cols_for_imputation)}\",\n",
    "        f\"Columns: {', '.join(categorical_cols_for_imputation)}\",\n",
    "        'Checked/Applied during imputation and reconstruction',\n",
    "        'New column \"DaysSinceJoin\"',\n",
    "        f\"Columns: {', '.join(categorical_cols_for_encoding)}. Added {len(encoded_col_names)} new columns.\",\n",
    "        f\"Columns: {', '.join(numerical_cols_for_scaling)}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "\n",
    "# Output the transformed DataFrame and the summary\n",
    "output = {\n",
    "    'Transformed Data Head': df_transformed_final.head(),\n",
    "    'Transformed Data Info (dtypes, non-null counts)': df_transformed_final.info(verbose=False, memory_usage=False), # info doesn't return a df, just prints\n",
    "    'Transformation Summary': transformation_summary,\n",
    "    'Transformed Data Describe': df_transformed_final.describe(include='all') # describe is useful post-transformation\n",
    "}\n",
    "\n",
    "# For 'info', need to capture output if possible, or instruct user.\n",
    "# Simple print will go to the console/error details. Let's use a string summary instead of info()\n",
    "info_string = df_transformed_final.info(verbose=True, memory_usage=False, buf=None) # buf=None prints to stdout\n",
    "\n",
    "# Let's add dtypes and non-null counts as DataFrames\n",
    "dtypes_df = df_transformed_final.dtypes.reset_index()\n",
    "dtypes_df.columns = ['Column', 'Dtype']\n",
    "\n",
    "non_null_counts = df_transformed_final.notnull().sum().reset_index()\n",
    "non_null_counts.columns = ['Column', 'NonNullCount']\n",
    "non_null_counts['TotalRows'] = len(df_transformed_final)\n",
    "non_null_counts['NullCount'] = non_null_counts['TotalRows'] - non_null_counts['NonNullCount']\n",
    "\n",
    "\n",
    "output = {\n",
    "    'Transformed Data Head': df_transformed_final.head(),\n",
    "    'Transformed Data Dtypes': dtypes_df,\n",
    "    'Transformed Data Non-Null Counts': non_null_counts,\n",
    "    'Transformation Summary': transformation_summary,\n",
    "    'Transformed Data Describe': df_transformed_final.describe(include='all')\n",
    "}\n",
    "\n",
    "\n",
    "output # Output the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c43cf",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "*   We load the raw data (`df_raw`) and create a copy (`df_transformed`) to avoid modifying the original Excel data.\n",
    "*   **Handle Duplicates:** `drop_duplicates()` is used to remove identical rows.\n",
    "*   **Handle Missing Values:** `sklearn.impute.SimpleImputer` is used.\n",
    "    *   We impute numerical columns (`PurchaseAmount`, `Rating`, `NumberOfVisits`) with the median value.\n",
    "    *   We impute the `IsLoyaltyMember` column (which might be read as object/string due to `None`) with the most frequent value.\n",
    "    *   `ColumnTransformer` is used to apply different imputation strategies to different subsets of columns, while `remainder='passthrough'` keeps other columns untouched during this specific step.\n",
    "    *   We reconstruct the DataFrame after imputation, paying attention to column order and type casting.\n",
    "*   **Correct Data Types:** Explicit `astype()` or `pd.to_numeric`/`pd.to_datetime` with `errors='coerce'` is used to ensure columns have the appropriate data types after transformations, as scikit-learn can sometimes return numpy arrays with generic types.\n",
    "*   **Feature Engineering:** A new column `DaysSinceJoin` is created by calculating the difference between a fixed analysis date and the `JoinDate`. We handle missing `JoinDate` values by assigning a sentinel value (-1).\n",
    "*   **Categorical Encoding:** `sklearn.preprocessing.OneHotEncoder` is used to convert categorical columns (`ProductCategory`, `Region`) into numerical format suitable for many models. Each unique value becomes a new binary column (one-hot encoding). `handle_unknown='ignore'` prevents errors if a new category appears later. `sparse_output=False` (or `sparse=False` in older versions) ensures the output is a dense numpy array.\n",
    "*   **Numerical Scaling:** `sklearn.preprocessing.StandardScaler` is used to standardize selected numerical columns (`PurchaseAmount`, `Rating`, `NumberOfVisits`). Standardization scales data to have a mean of 0 and a standard deviation of 1, which is important for distance-based algorithms like K-Means or SVMs.\n",
    "*   We generate a summary DataFrame detailing the transformations performed and return various views of the final `df_transformed_final` DataFrame, including its head, data types, non-null counts, and a summary description.\n",
    "\n",
    "**Viewing the Output:**\n",
    "\n",
    "*   Click the Python cell, then click the Python icon/button next to the formula bar.\n",
    "*   Select \"Excel Value\" (**Ctrl+Shift+Alt+M**) for the DataFrames within the output dictionary ('Transformed Data Head', 'Transformed Data Dtypes', 'Transformed Data Non-Null Counts', 'Transformation Summary', 'Transformed Data Describe') to spill them into your sheet.\n",
    "\n",
    "The output DataFrame `df_transformed_final` is now cleaned and transformed, ready for analysis or modeling steps. The next topic in the series is [\"Data Cleaning & Preparation - 3. Data Integration\"](./03-Data%20Cleaning%20&%20Preparation_03-Data%20Integration.md), which explores techniques for combining data from multiple sources while maintaining data quality.\n",
    "\n",
    "**Further Analysis:**\n",
    "* **Advanced Feature Engineering:** Using polynomial features, interaction terms, and custom transformers in scikit-learn pipelines\n",
    "* **Automated Feature Selection:** Implementing LASSO, Ridge regression, or tree-based feature importance for dimensionality reduction\n",
    "* **Custom Encoders:** Creating specialized encoders for domain-specific categorical variables or ordinal relationships\n",
    "* **Advanced Imputation:** Using iterative imputation (MICE) or KNN imputation for complex missing data patterns\n",
    "* **Transformation Validation:** Implementing cross-validation for transformation pipelines to ensure robustness"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
