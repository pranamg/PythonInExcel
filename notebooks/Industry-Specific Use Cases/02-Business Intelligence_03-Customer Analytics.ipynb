{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b88b52",
   "metadata": {},
   "source": [
    "The final topic in the business intelligence series is **Business Intelligence - 3. Customer Analytics**.\n",
    "\n",
    "Customer Analytics is a critical component for understanding customer behavior, identifying high-value customers, and developing targeted strategies. This section covers essential techniques including RFM analysis, customer segmentation, and basic sentiment analysis.\n",
    "\n",
    "Based on [`piplist.txt`](./README.md) output, you have libraries needed for these tasks including `pandas`, `numpy`, `scikit-learn` (for clustering), `nltk` (for text analysis), and `seaborn`/`matplotlib` (for visualization).\n",
    "\n",
    "**Step 1: Generate Sample Customer Transaction Data**\n",
    "\n",
    "We'll create dummy data representing customer transactions over time, including a simple feedback field.\n",
    "\n",
    "In a new Excel cell, enter `=PY` and paste the following code, then press **Ctrl+Enter**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1fff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy customer transaction data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import date, timedelta\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "num_customers = 300\n",
    "num_transactions_total = 8000 # Total number of transactions\n",
    "start_date = date(2022, 1, 1)\n",
    "end_date = date(2024, 5, 15) # A specific 'analysis date' reference point\n",
    "transaction_dates = pd.to_datetime(pd.date_range(start=start_date, end=end_date, periods=num_transactions_total)) # Evenly distributed dates\n",
    "\n",
    "customer_ids = [f'Cust_{i}' for i in range(num_customers)]\n",
    "\n",
    "data = []\n",
    "\n",
    "# Generate transactions\n",
    "for transaction_date in transaction_dates:\n",
    "    customer_id = random.choice(customer_ids)\n",
    "    amount = round(random.uniform(10, 500), 2) # Simulate varied transaction amounts\n",
    "\n",
    "    # Simulate some simple feedback text\n",
    "    feedback_choice = random.choices(['positive', 'negative', 'neutral', 'empty'], weights=[0.3, 0.15, 0.45, 0.1], k=1)[0]\n",
    "    feedback = \"\"\n",
    "    if feedback_choice == 'positive':\n",
    "        feedback = random.choice([\"Great product!\", \"Very happy with the service.\", \"Highly recommend.\", \"Fantastic experience.\", \"Will buy again.\"])\n",
    "    elif feedback_choice == 'negative':\n",
    "        feedback = random.choice([\"Disappointed.\", \"Slow delivery.\", \"Issue with the item.\", \"Poor customer support.\", \"Not satisfied.\"])\n",
    "    elif feedback_choice == 'neutral':\n",
    "        feedback = random.choice([\"Product is okay.\", \"Arrived on time.\", \"No issues.\", \"Met expectations.\"])\n",
    "\n",
    "    data.append([customer_id, transaction_date, amount, feedback])\n",
    "\n",
    "df_customers = pd.DataFrame(data, columns=['CustomerID', 'TransactionDate', 'Amount', 'Feedback'])\n",
    "\n",
    "# Add a transaction ID for uniqueness\n",
    "df_customers['TransactionID'] = df_customers.index + 1\n",
    "\n",
    "# Randomize row order after generation\n",
    "df_customers = df_customers.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_customers # Output the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751ff0bb",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "*   This code generates a DataFrame `df_customers` with simulated transaction data for multiple customers.\n",
    "*   It includes `CustomerID`, `TransactionDate`, `Amount`, and a simple `Feedback` text field.\n",
    "*   `Faker` is used for generic data simulation setup.\n",
    "*   It distributes transactions over a period, ensuring customers have varying numbers of transactions and different last purchase dates and total spending.\n",
    "*   It generates simple positive, negative, or neutral feedback text.\n",
    "*   The result, `df_customers`, will be spilled into your Excel sheet. Let's assume this data is placed in a range or Table named `CustomerTransactions`.\n",
    "\n",
    "**Step 2: Perform RFM Analysis, Segmentation, Sentiment Analysis, and Visualize**\n",
    "\n",
    "Now, we'll calculate RFM metrics, segment customers based on these metrics, perform sentiment analysis on feedback, and visualize the results.\n",
    "\n",
    "In a **new** Excel cell, enter `=PY` and paste the following code. Replace `\"CustomerTransactions\"` with the actual name of the Excel range/Table where your dummy data is. Press **Ctrl+Enter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc591ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform RFM analysis, segmentation, sentiment analysis, and visualize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date, timedelta\n",
    "from sklearn.preprocessing import StandardScaler # For scaling RFM values\n",
    "from sklearn.cluster import KMeans # For K-Means clustering\n",
    "import nltk\n",
    "try: nltk.data.find('sentiment/vader_lexicon.zip'); print('VADER lexicon found.')\n",
    "except: nltk.download('vader_lexicon'); print('VADER lexicon downloaded.')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # For sentiment analysis\n",
    "\n",
    "# Ensure NLTK data is available (VADER lexicon).\n",
    "# In Python in Excel, this might be pre-downloaded. If not, this command might fail.\n",
    "# You can try running this in a separate cell first if needed:\n",
    "# =PY(import nltk; try: nltk.data.find('sentiment/vader_lexicon.zip'); print('VADER lexicon found.') except: nltk.download('vader_lexicon'); print('VADER lexicon downloaded.'))\n",
    "\n",
    "\n",
    "# Load the transaction data from Excel\n",
    "# IMPORTANT: Replace \"CustomerTransactions\" with the actual name of your Excel range or Table\n",
    "df_customers = xl(\"CustomerTransactions[#All]\", headers=True)\n",
    "\n",
    "# Ensure 'TransactionDate' is a datetime column\n",
    "df_customers['TransactionDate'] = pd.to_datetime(df_customers['TransactionDate'])\n",
    "df_customers['CustomerID'] = df_customers['CustomerID'].astype(str)\n",
    "df_customers['Amount'] = pd.to_numeric(df_customers['Amount'])\n",
    "\n",
    "\n",
    "# --- RFM Analysis ---\n",
    "\n",
    "# Define a snapshot date (e.g., the day after the last transaction date)\n",
    "snapshot_date = df_customers['TransactionDate'].max() + timedelta(days=1)\n",
    "\n",
    "# Calculate RFM metrics\n",
    "rfm_df = df_customers.groupby('CustomerID').agg(\n",
    "    Recency=('TransactionDate', lambda date: (snapshot_date - date.max()).days), # Days since last transaction\n",
    "    Frequency=('TransactionID', 'count'), # Number of transactions\n",
    "    Monetary=('Amount', 'sum') # Total spending\n",
    ").reset_index()\n",
    "\n",
    "# Ensure Monetary is not zero for clustering (add epsilon or handle separately if needed)\n",
    "rfm_df['Monetary'] = rfm_df['Monetary'] + 1 # Simple add 1 to avoid log(0) if scaling later, or just handle 0s\n",
    "\n",
    "\n",
    "# --- Customer Segmentation (using K-Means on RFM) ---\n",
    "\n",
    "# Scale RFM values - necessary for K-Means as it's distance-based\n",
    "# Consider log scaling for highly skewed data like Monetary/Frequency, then Standardize\n",
    "rfm_scaled = rfm_df[['Recency', 'Frequency', 'Monetary']].copy()\n",
    "# Log transform for skewed distributions (handle Recency differently as lower is better)\n",
    "rfm_scaled['Frequency'] = np.log1p(rfm_scaled['Frequency']) # log(1+x)\n",
    "rfm_scaled['Monetary'] = np.log1p(rfm_scaled['Monetary'])\n",
    "# Recency is usually inverse, maybe log(max_recency - recency + 1) or just inverse scale\n",
    "# Let's just standardize the raw RFM values for simplicity first, after adding 1 to Monetary/Frequency\n",
    "# rfm_scaled = rfm_df[['Recency', 'Frequency', 'Monetary']].copy() # Use original if log scaling is too complex or data not skewed\n",
    "\n",
    "scaler = StandardScaler()\n",
    "rfm_normalized = scaler.fit_transform(rfm_scaled) # Scale the (potentially log-transformed) data\n",
    "\n",
    "# Determine optimal number of clusters (Elbow method requires plotting, let's pick a number)\n",
    "n_clusters = 4 # Example: 4 clusters (e.g., Champions, Loyal Customers, etc.)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10) # n_init to suppress warning\n",
    "\n",
    "rfm_df['Cluster'] = kmeans.fit_predict(rfm_normalized)\n",
    "\n",
    "# Analyze cluster characteristics (mean RFM values per cluster)\n",
    "cluster_summary = rfm_df.groupby('Cluster')[['Recency', 'Frequency', 'Monetary']].mean().reset_index()\n",
    "# Sort clusters based on average Monetary value to get a rough ordering (e.g., 0 being high value)\n",
    "# cluster_summary = cluster_summary.sort_values('Monetary', ascending=False) # Optional sorting\n",
    "\n",
    "# Map cluster IDs to potential names based on summary (manual inspection needed)\n",
    "# Example mapping based on typical RFM patterns (High F/M, Low R = Champions)\n",
    "# This mapping logic would ideally be based on analyzing the cluster_summary output\n",
    "# Simple mapping based on sorting might assign 0 to the highest Monetary cluster\n",
    "# Let's just keep them as numerical IDs for now, but print the summary\n",
    "# rfm_df['Cluster_Name'] = rfm_df['Cluster'].map(...) # Requires cluster_summary analysis\n",
    "\n",
    "\n",
    "# --- Sentiment Analysis on Feedback ---\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get sentiment score\n",
    "def get_sentiment_score(text):\n",
    "    if pd.isna(text) or text.strip() == \"\":\n",
    "        return None # Handle missing or empty feedback\n",
    "    return analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "# Apply sentiment analysis\n",
    "df_customers['Sentiment_Score'] = df_customers['Feedback'].apply(get_sentiment_score)\n",
    "\n",
    "# Classify sentiment\n",
    "def classify_sentiment(score):\n",
    "    if score is None:\n",
    "        return 'No Feedback'\n",
    "    elif score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df_customers['Sentiment_Category'] = df_customers['Sentiment_Score'].apply(classify_sentiment)\n",
    "\n",
    "# Summarize sentiment distribution\n",
    "sentiment_counts = df_customers['Sentiment_Category'].value_counts().reset_index()\n",
    "sentiment_counts.columns = ['Sentiment Category', 'Count']\n",
    "\n",
    "\n",
    "# --- Visualization ---\n",
    "\n",
    "# Apply custom style guidelines\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.edgecolor'] = '#1a1a24'\n",
    "plt.rcParams['axes.linewidth'] = 1\n",
    "plt.rcParams['axes.grid'] = False # Turn off default grid\n",
    "sns.set_theme(style=\"whitegrid\") # Use a seaborn theme base, then apply customs\n",
    "\n",
    "# 1. RFM Distribution Histograms\n",
    "fig1, axes = plt.subplots(1, 3, figsize=(15, 5)) # One row, three columns for R, F, M\n",
    "\n",
    "sns.histplot(rfm_df['Recency'], bins=30, kde=True, ax=axes[0], color='#ff6d00') # Orange\n",
    "axes[0].set_title('Recency Distribution', fontsize=14, color='#1a1a24')\n",
    "axes[0].set_xlabel('Days Since Last Purchase', fontsize=12, color='#1a1a24')\n",
    "axes[0].set_ylabel('Number of Customers', fontsize=12, color='#1a1a24')\n",
    "sns.despine(ax=axes[0], top=True, right=True)\n",
    "axes[0].grid(False)\n",
    "\n",
    "\n",
    "sns.histplot(rfm_df['Frequency'], bins=30, kde=True, ax=axes[1], color='#188ce5') # Blue\n",
    "axes[1].set_title('Frequency Distribution', fontsize=14, color='#1a1a24')\n",
    "axes[1].set_xlabel('Number of Transactions', fontsize=12, color='#1a1a24')\n",
    "axes[1].set_ylabel('', fontsize=12, color='#1a1a24') # Share Y-axis label with the first plot\n",
    "sns.despine(ax=axes[1], top=True, right=True)\n",
    "axes[1].grid(False)\n",
    "\n",
    "sns.histplot(rfm_df['Monetary'], bins=30, kde=True, ax=axes[2], color='#2db757') # Green\n",
    "axes[2].set_title('Monetary Distribution', fontsize=14, color='#1a1a24')\n",
    "axes[2].set_xlabel('Total Spending', fontsize=12, color='#1a1a24')\n",
    "axes[2].set_ylabel('', fontsize=12, color='#1a1a24') # Share Y-axis label\n",
    "sns.despine(ax=axes[2], top=True, right=True)\n",
    "axes[2].grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# 2. Customer Segmentation Scatter Plot (Frequency vs Monetary, colored by cluster)\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "# Use original (unscaled) values for plotting, but color by cluster from scaled data\n",
    "sns.scatterplot(x='Frequency', y='Monetary', hue='Cluster', data=rfm_df, palette='viridis', s=50, ax=ax2) # 'viridis' is a common colormap\n",
    "\n",
    "ax2.set_title(f'Customer Segmentation by RFM ({n_clusters} Clusters)', fontsize=14, color='#1a1a24')\n",
    "ax2.set_xlabel('Frequency (Number of Transactions)', fontsize=12, color='#1a1a24')\n",
    "ax2.set_ylabel('Monetary (Total Spending)', fontsize=12, color='#1a1a24')\n",
    "ax2.grid(False)\n",
    "sns.despine(ax=ax2, top=True, right=True)\n",
    "\n",
    "# Add cluster centers to the plot (requires inverse transform if scaled)\n",
    "# kmeans.cluster_centers_ are in the scaled space. Need to inverse transform.\n",
    "# cluster_centers_original_scale = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "# ax2.scatter(cluster_centers_original_scale[:, 1], cluster_centers_original_scale[:, 2], s=300, c='red', marker='X', label='Cluster Centers') # [:,1] for Frequency, [:,2] for Monetary\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# 3. Sentiment Distribution Bar Chart\n",
    "fig3, ax3 = plt.subplots(figsize=(8, 5))\n",
    "# Order bars logically\n",
    "sentiment_order = ['Positive', 'Neutral', 'Negative', 'No Feedback']\n",
    "# Using a categorical color palette - let's map colors to sentiment\n",
    "sentiment_colors = {'Positive': '#2db757', 'Neutral': '#188ce5', 'Negative': '#ff4136', 'No Feedback': '#750e5c'} # Green, Blue, Salmon, Purple\n",
    "\n",
    "# Ensure all categories are in the DataFrame for plotting consistency\n",
    "sentiment_counts = sentiment_counts.set_index('Sentiment Category').reindex(sentiment_order).fillna(0).reset_index()\n",
    "\n",
    "\n",
    "sns.barplot(x='Count', y='Sentiment Category', hue = 'Sentiment Category', legend=False, data=sentiment_counts, ax=ax3, palette=[sentiment_colors[cat] for cat in sentiment_counts['Sentiment Category']])\n",
    "\n",
    "\n",
    "ax3.set_title('Customer Feedback Sentiment Distribution', fontsize=14, color='#1a1a24')\n",
    "ax3.set_xlabel('Number of Feedbacks', fontsize=12, color='#1a1a24')\n",
    "ax3.set_ylabel('Sentiment', fontsize=12, color='#1a1a24')\n",
    "sns.despine(ax=ax3, top=True, right=True)\n",
    "ax3.grid(False)\n",
    "\n",
    "# Add data labels (counts)\n",
    "for index, row in sentiment_counts.iterrows():\n",
    "     ax3.text(row['Count'], index, f' {int(row[\"Count\"]):,}', color='#1a1a24', va='center')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Output results\n",
    "output = {\n",
    "    'RFM Metrics Head': rfm_df.head(),\n",
    "    'Customer Segmentation (with Clusters) Head': rfm_df[['CustomerID', 'Recency', 'Frequency', 'Monetary', 'Cluster']].head(),\n",
    "    'Cluster Summary (Mean RFM per Cluster)': cluster_summary,\n",
    "    'Sentiment Analysis Results Head': df_customers[['TransactionID', 'CustomerID', 'Feedback', 'Sentiment_Score', 'Sentiment_Category']].head(),\n",
    "    'Sentiment Distribution Counts': sentiment_counts,\n",
    "    'RFM_Distribution_Histograms': fig1,\n",
    "    'Customer_Segmentation_Scatter_Plot': fig2,\n",
    "    'Sentiment_Distribution_Bar_Chart': fig3,\n",
    "}\n",
    "\n",
    "output # Output the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b8f62",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "*   We load the dummy transaction data. **Remember to replace `\"CustomerTransactions\"`**.\n",
    "*   We calculate Recency, Frequency, and Monetary (RFM) values for each customer using `groupby()` and aggregation. Recency is calculated relative to a defined `snapshot_date`.\n",
    "*   We prepare the RFM data for clustering by scaling the values. Scaling is important for K-Means as it's based on distances. We use `StandardScaler`.\n",
    "*   We apply K-Means clustering with a predefined number of clusters (`n_clusters = 4`) to segment customers based on their scaled RFM values.\n",
    "*   We calculate the mean RFM values for each cluster to understand the characteristics of each segment.\n",
    "*   We perform sentiment analysis on the 'Feedback' column using `nltk.sentiment.vader`. This assigns a compound score to each feedback string. We then classify the score into 'Positive', 'Negative', 'Neutral', or 'No Feedback'.\n",
    "*   We count the occurrences of each sentiment category.\n",
    "*   **Visualization:**\n",
    "    *   `fig1`: Three histograms showing the distribution of Recency, Frequency, and Monetary values across all customers.\n",
    "    *   `fig2`: A scatter plot showing customers based on their Frequency and Monetary values, with points colored according to their assigned cluster. This helps visualize the segmentation.\n",
    "    *   `fig3`: A horizontal bar chart showing the count of feedbacks in each sentiment category. Data labels are included.\n",
    "*   **Custom Style:** Applied the specified style guidelines (font, colors - using different colors for each RFM histogram and mapping colors to sentiment categories, axes, spines, grid, data labels, negative number format - although not applicable here).\n",
    "*   We return a dictionary containing heads of calculated DataFrames/Series (RFM, Segmentation, Sentiment), the Cluster Summary, Sentiment Counts, and the three plot figures.\n",
    "\n",
    "**Viewing the Output:**\n",
    "\n",
    "*   Click the Python cell, then click the Python icon/button next to the formula bar.\n",
    "*   Select \"Excel Value\" (**Ctrl+Shift+Alt+M**) for the DataFrames/Series ('RFM Metrics Head', 'Customer Segmentation (with Clusters) Head', 'Cluster Summary (Mean RFM per Cluster)', 'Sentiment Analysis Results Head', 'Sentiment Distribution Counts') to spill them into your sheet.\n",
    "*   For each plot figure object ('RFM_Distribution_Histograms', 'Customer_Segmentation_Scatter_Plot', 'Sentiment_Distribution_Bar_Chart'), select \"Picture in Cell\" > \"Create Reference\" to see the plots.\n",
    "\n",
    "This analysis provides foundational insights into customer behavior and segmentation. The next major category in the series is [**Data Cleaning & Preparation**](./03-Data%20Cleaning%20&%20Preparation_01-Basic%20Data%20Cleaning.md), which covers essential techniques for ensuring data quality and consistency.\n",
    "\n",
    "**Further Analysis:**\n",
    "* **Churn Prediction:** Using scikit-learn's survival analysis or logistic regression to predict customer churn based on RFM metrics and sentiment scores\n",
    "* **Customer Lifetime Value (CLV):** Implementing probabilistic models to estimate future customer value using purchase patterns and monetary value\n",
    "* **Market Basket Analysis:** Using the Apriori algorithm or association rules mining to identify product purchase patterns and recommend cross-selling opportunities\n",
    "* **Advanced Text Analytics:** Applying topic modeling (LDA) or word embeddings to extract deeper insights from customer feedback\n",
    "* **Customer Journey Analysis:** Creating directed graphs of customer touchpoints to analyze common paths and identify optimization opportunities"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
