{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1928139",
   "metadata": {},
   "source": [
    "# Leveraging Dask in Python in Excel\n",
    "\n",
    "Dask unlocks parallel and out-of-core computing for large datasets, complementing Python in Excel’s capabilities by enabling you to process data that exceeds in-memory limits and accelerate computations across multiple cores or machines. Below is a structured guide to harnessing Dask within the Python in Excel environment.\n",
    "\n",
    "## 1. Setup and Imports\n",
    "\n",
    "Before using Dask, reserve the first worksheet in your workbook for import statements to ensure they run before any analysis steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1fa331",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7325ab7",
   "metadata": {},
   "source": [
    "This imports Dask’s DataFrame API (`dd`) and the distributed scheduler client.\n",
    "\n",
    "## 2. Initializing a Dask Client\n",
    "\n",
    "Starting a local or remote Dask client provides a dashboard for monitoring parallel tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b1a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "client = Client(n_workers=2, threads_per_worker=2, memory_limit='2GB')\n",
    "client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec3ff47",
   "metadata": {},
   "source": [
    "This creates a local cluster with two workers, each having two threads and 2 GB memory, and returns a link to the Dask dashboard.\n",
    "\n",
    "## 3. Reading Large Datasets\n",
    "\n",
    "Use Dask’s `read_*` functions to lazily load large files without reading them fully into memory:\n",
    "\n",
    "- **CSV Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "df = dd.read_csv('LargeData.csv', assume_missing=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e9f422",
   "metadata": {},
   "source": [
    "- **Parquet Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc0ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "df = dd.read_parquet('DatasetFolder/')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3db1e9",
   "metadata": {},
   "source": [
    "`assume_missing=True` helps Dask infer nullable integer columns correctly.\n",
    "\n",
    "## 4. Working with Dask DataFrames\n",
    "\n",
    "Dask DataFrames mirror Pandas APIs but operate on partitioned data:\n",
    "\n",
    "- **Inspect Partitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f49f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "df.npartitions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a520990d",
   "metadata": {},
   "source": [
    "- **Compute Descriptive Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd53ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "df.describe().compute()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa3552d",
   "metadata": {},
   "source": [
    "- **Filtering and Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aabd8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "high_sales = df[df['Sales'] > 1000]\n",
    "high_sales.head().compute()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098cfa1f",
   "metadata": {},
   "source": [
    "Methods like `head()`, `describe()`, and Boolean indexing behave similarly to Pandas but require `.compute()` to execute and retrieve results.\n",
    "\n",
    "## 5. Aggregations and GroupBy\n",
    "\n",
    "Perform group-wise computations in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cb22a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "grouped = df.groupby('Region').agg({'Revenue': 'sum', 'Units': 'mean'})\n",
    "grouped.compute()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb230e",
   "metadata": {},
   "source": [
    "This operation splits data by `Region`, aggregates partitions independently, and merges results upon `compute()`.\n",
    "\n",
    "## 6. Integrating with Excel Data\n",
    "\n",
    "Combine `xl()` references with Dask operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3e54c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "import pandas as pd\n",
    "# Load a DataFrame from Excel range\n",
    "pdf = pd.DataFrame(xl(\"A1:C1000\", headers=True))\n",
    "# Convert to Dask DataFrame with 4 partitions\n",
    "ddf = dd.from_pandas(pdf, npartitions=4)\n",
    "ddf['ValueSquared'] = ddf['Value'] ** 2\n",
    "ddf.compute()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a4766",
   "metadata": {},
   "source": [
    "This workflow bridges Excel ranges and Dask’s parallel engine, allowing transformations on imported data.\n",
    "\n",
    "## 7. Visualization and Output\n",
    "\n",
    "After computing results, spill them back into Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed28528",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "result = df[df['Category']=='A'].compute()\n",
    "result\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef10c77e",
   "metadata": {},
   "source": [
    "Right-click the PY icon in the cell and choose **Output As → Excel Values** to spill the DataFrame into the grid.\n",
    "\n",
    "## 8. Best Practices\n",
    "\n",
    "- **Lazy Evaluation**: Chain operations without `.compute()` until final step to optimize task graphs.\n",
    "- **Partition Management**: Adjust `npartitions` to balance between parallelism overhead and memory constraints.\n",
    "- **Dashboard Monitoring**: Use the Dask dashboard to inspect task progress, memory usage, and diagnose bottlenecks.\n",
    "- **Chunked Inputs**: For very large raw data, prefer Parquet or chunked CSV reads (`blocksize` parameter) to improve I/O efficiency.\n",
    "\n",
    "By integrating Dask with Python in Excel, you can scale analyses to datasets that exceed in-memory limits, leverage multiple cores or machines, and maintain familiar Excel-based workflows."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
