{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b35bdd1",
   "metadata": {},
   "source": [
    "# Leveraging NLTK Library in Python in Excel\n",
    "\n",
    "Building on our previous discussions about Python libraries in Excel, the Natural Language Toolkit (NLTK) represents one of the most powerful text analysis capabilities available in the Python in Excel environment. NLTK brings sophisticated natural language processing capabilities directly into your familiar Excel interface, enabling you to perform complex text analysis tasks without leaving your spreadsheet environment.\n",
    "\n",
    "## Understanding NLTK Availability in Python in Excel\n",
    "\n",
    "Python in Excel comes with NLTK pre-installed through the Anaconda distribution, making it readily accessible for text analysis tasks [^5_1]. The integration includes several pre-loaded corpora that are essential for natural language processing: brown, punkt, stopwords, treebank, vader, and wordnet2022 [^5_2]. This means you can immediately begin performing text analysis without additional downloads or installations.\n",
    "\n",
    "## Getting Started with NLTK in Excel\n",
    "\n",
    "### Basic Import and Setup\n",
    "\n",
    "To begin using NLTK in Python in Excel, you'll start with import statements in a Python cell. Place these imports on the first worksheet of your workbook to ensure they're available throughout your analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57e8a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ff24e8",
   "metadata": {},
   "source": [
    "The beauty of Python in Excel is that once you've imported these modules, they remain available throughout your workbook for subsequent Python formulas [^5_1].\n",
    "\n",
    "## Text Preprocessing with NLTK\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Tokenization is the fundamental first step in text analysis, breaking down text into individual words or sentences [^5_3]. NLTK provides several tokenization methods that work seamlessly with Excel data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a5d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "text_data = xl(\"A1\")\n",
    "tokens = word_tokenize(text_data)\n",
    "tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a1d3d8",
   "metadata": {},
   "source": [
    "For sentence-level tokenization, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1b0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "text_data = xl(\"TextColumn[#All]\", headers=True)\n",
    "sentences = [sent_tokenize(text) for text in text_data]\n",
    "sentences\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb44586",
   "metadata": {},
   "source": [
    "This approach allows you to process entire columns of text data efficiently, creating tokenized versions of your content for further analysis.\n",
    "\n",
    "### Stop Words Removal\n",
    "\n",
    "Stop words are common words that typically don't contribute meaningful information to text analysis [^5_4]. NLTK's pre-loaded stopwords corpus makes it easy to filter these out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc539c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text_data = xl(\"B2\")\n",
    "tokens = word_tokenize(text_data.lower())\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "filtered_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad555252",
   "metadata": {},
   "source": [
    "This filtering process is particularly valuable when analyzing large datasets, as it helps focus on the most meaningful content words [^5_5].\n",
    "\n",
    "### Stemming and Lemmatization\n",
    "\n",
    "NLTK provides both stemming and lemmatization capabilities for normalizing words to their root forms [^5_2][^5_6]. Stemming uses algorithmic approaches to remove suffixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7317a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "words = xl(\"WordList[#All]\", headers=True)\n",
    "stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "pd.DataFrame({'Original': words, 'Stemmed': stemmed_words})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233279d",
   "metadata": {},
   "source": [
    "Lemmatization provides more sophisticated word normalization based on linguistic knowledge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8725df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = xl(\"WordList[#All]\", headers=True)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "pd.DataFrame({'Original': words, 'Lemmatized': lemmatized_words})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6bf6df",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with NLTK\n",
    "\n",
    "### VADER Sentiment Analysis\n",
    "\n",
    "One of the most practical applications of NLTK in Excel is sentiment analysis using the VADER (Valence Aware Dictionary and sEntiment Reasoner) analyzer [^5_7][^5_8]. VADER is particularly effective for social media text and informal language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca4683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "text_data = xl(\"ReviewText[#All]\", headers=True)\n",
    "sentiment_scores = []\n",
    "for text in text_data:\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    sentiment_scores.append(scores)\n",
    "pd.DataFrame(sentiment_scores)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a7c21e",
   "metadata": {},
   "source": [
    "VADER returns four metrics: negative, neutral, positive, and compound scores [^5_8]. The compound score ranges from -1 (highly negative) to +1 (highly positive), making it easy to classify sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5394e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "def classify_sentiment(compound_score):\n",
    "    if compound_score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "sentiment_data = xl(\"SentimentScores[compound]\")\n",
    "classifications = [classify_sentiment(score) for score in sentiment_data]\n",
    "classifications\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba311857",
   "metadata": {},
   "source": [
    "This classification system enables you to automatically categorize large volumes of text data based on emotional tone [^5_9].\n",
    "\n",
    "## Text Analysis and Frequency Distribution\n",
    "\n",
    "### Word Frequency Analysis\n",
    "\n",
    "NLTK's FreqDist class provides powerful tools for analyzing word frequencies in your text data [^5_10]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1504689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "from nltk import FreqDist\n",
    "text_data = xl(\"DocumentText[#All]\", headers=True)\n",
    "all_tokens = []\n",
    "for text in text_data:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [token for token in tokens if token.isalpha()]\n",
    "    all_tokens.extend(filtered_tokens)\n",
    "\n",
    "freq_dist = FreqDist(all_tokens)\n",
    "top_words = freq_dist.most_common(20)\n",
    "pd.DataFrame(top_words, columns=['Word', 'Frequency'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47488340",
   "metadata": {},
   "source": [
    "This analysis helps identify the most common terms in your dataset, providing insights into dominant themes and topics [^5_10].\n",
    "\n",
    "### N-gram Analysis\n",
    "\n",
    "Beyond individual words, NLTK enables analysis of phrase patterns through n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec7b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "from nltk import bigrams, trigrams\n",
    "text_data = xl(\"A1\")\n",
    "tokens = word_tokenize(text_data.lower())\n",
    "bigram_freq = FreqDist(bigrams(tokens))\n",
    "trigram_freq = FreqDist(trigrams(tokens))\n",
    "\n",
    "bigram_results = bigram_freq.most_common(10)\n",
    "trigram_results = trigram_freq.most_common(10)\n",
    "pd.DataFrame({\n",
    "    'Bigrams': [' '.join(bg) for bg, freq in bigram_results],\n",
    "    'Bigram_Freq': [freq for bg, freq in bigram_results]\n",
    "})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861bb09e",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging and Named Entity Recognition\n",
    "\n",
    "### POS Tagging\n",
    "\n",
    "NLTK's part-of-speech tagging capabilities help identify grammatical roles of words in text [^5_11]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f44dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "from nltk import pos_tag\n",
    "text_data = xl(\"B2\")\n",
    "tokens = word_tokenize(text_data)\n",
    "pos_tags = pos_tag(tokens)\n",
    "pd.DataFrame(pos_tags, columns=['Word', 'POS_Tag'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a4015",
   "metadata": {},
   "source": [
    "This functionality is particularly useful for filtering specific word types or analyzing grammatical patterns in your text [^5_12].\n",
    "\n",
    "### Named Entity Recognition\n",
    "\n",
    "NLTK provides named entity recognition to identify people, organizations, locations, and other entities in text [^5_13]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab9a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "from nltk import ne_chunk\n",
    "text_data = xl(\"NewsArticle[#All]\", headers=True)\n",
    "entities = []\n",
    "for text in text_data:\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    ne_tree = ne_chunk(tagged)\n",
    "    entities.extend([\n",
    "        (word, 'ORG') if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION' else (word, 'PERSON')\n",
    "        for chunk in ne_tree\n",
    "        for word, pos in chunk\n",
    "    ])\n",
    "\n",
    "pd.DataFrame(entities, columns=['Entity', 'Type'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c0091a",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "### Building Custom Classifiers\n",
    "\n",
    "NLTK supports various classification algorithms for text categorization tasks [^5_14]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b80262",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def extract_features(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return {word: True for word in filtered_tokens}\n",
    "\n",
    "# Prepare training data\n",
    "training_data = xl(\"TrainingData[#All]\", headers=True)\n",
    "feature_sets = [(extract_features(text), label) for text, label in training_data]\n",
    "\n",
    "# Train classifier\n",
    "classifier = NaiveBayesClassifier.train(feature_sets)\n",
    "\n",
    "# Classify new text\n",
    "new_text = xl(\"NewTextData[#All]\", headers=True)\n",
    "predictions = [classifier.classify(extract_features(text)) for text in new_text]\n",
    "predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75c7611",
   "metadata": {},
   "source": [
    "## Concordance and Context Analysis\n",
    "\n",
    "### Text Concordance\n",
    "\n",
    "NLTK's concordance functionality helps you find contexts where specific words appear [^5_15]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ebb1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "from nltk.text import Text\n",
    "text_data = xl(\"DocumentText[#All]\", headers=True)\n",
    "combined_text = ' '.join(text_data)\n",
    "tokens = word_tokenize(combined_text)\n",
    "text_obj = Text(tokens)\n",
    "\n",
    "# Find concordance lines for a specific word\n",
    "target_word = \"innovation\"\n",
    "concordance_lines = text_obj.concordance_list(target_word, width=80, lines=25)\n",
    "pd.DataFrame([(line.left, line.query, line.right) for line in concordance_lines], \n",
    "             columns=['Left_Context', 'Target_Word', 'Right_Context'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48515bf7",
   "metadata": {},
   "source": [
    "## Best Practices for NLTK in Excel\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "When working with large datasets, consider processing text in batches to maintain responsive performance [^5_12]. Break down complex operations into multiple cells to leverage Excel's calculation order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ce47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "# Cell 1: Load and tokenize data\n",
    "text_data = xl(\"LargeDataset[Text]\")\n",
    "tokenized_data = [word_tokenize(text.lower()) for text in text_data]\n",
    "tokenized_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe88682",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae11925",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "# Cell 2: Remove stop words (reference previous cell)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "cleaned_tokens = []\n",
    "for tokens in xl(\"PreviousCell\"):  # Reference the tokenized data\n",
    "    filtered = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
    "    cleaned_tokens.append(filtered)\n",
    "cleaned_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994d7ea7",
   "metadata": {},
   "source": [
    "### Data Integration\n",
    "\n",
    "NLTK works seamlessly with pandas DataFrames in Excel, making it easy to integrate text analysis results with your existing data workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a23d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "=PY(\n",
    "# Combine multiple text analysis results\n",
    "original_data = xl(\"OriginalTable[#All]\", headers=True)\n",
    "sentiment_scores = xl(\"SentimentResults[compound]\")\n",
    "word_counts = xl(\"WordCountResults[count]\")\n",
    "\n",
    "analysis_df = pd.DataFrame({\n",
    "    'Text': original_data['text_column'],\n",
    "})\n",
    "analysis_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed72305f",
   "metadata": {},
   "source": [
    "## Practical Applications\n",
    "\n",
    "### Business Intelligence\n",
    "\n",
    "NLTK in Excel enables sophisticated business intelligence applications including customer feedback analysis, social media monitoring, and competitive intelligence gathering [^5_16]. The integration allows business analysts to perform advanced text analytics without specialized NLP software.\n",
    "\n",
    "### Research and Academic Applications\n",
    "\n",
    "Researchers can leverage NLTK's comprehensive toolkit for linguistic analysis, content analysis, and corpus linguistics studies directly within Excel's familiar environment [^5_17]. This democratizes access to advanced NLP techniques for users who may not be comfortable with command-line interfaces.\n",
    "\n",
    "### Content Analysis\n",
    "\n",
    "Marketing teams can analyze campaign effectiveness, brand sentiment, and content performance using NLTK's text analysis capabilities combined with Excel's visualization and reporting features [^5_18].\n",
    "\n",
    "The integration of NLTK with Python in Excel represents a significant advancement in making natural language processing accessible to a broader audience while maintaining the familiar Excel interface that millions of users rely on for data analysis and reporting tasks."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
