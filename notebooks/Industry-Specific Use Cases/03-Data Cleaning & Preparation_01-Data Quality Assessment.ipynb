{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f20fe7a",
   "metadata": {},
   "source": [
    "The next major category in this series is **Data Cleaning & Preparation**, beginning with **1. Data Quality Assessment**.\n",
    "\n",
    "Data Quality Assessment serves as the foundation for any reliable data analysis process. This essential step systematically identifies and evaluates data quality issues including missing values, duplicates, inconsistent data types, outliers, and structural errors in datasets.\n",
    "\n",
    "Based on [`piplist.txt`](./README.md) output, we have `pandas` (essential for this), `numpy`, and visualization libraries (`matplotlib`, `seaborn`), all of which are perfect for performing data quality checks.\n",
    "\n",
    "**Step 1: Generate Sample Data with Data Quality Issues**\n",
    "\n",
    "We'll create a dummy dataset that intentionally includes common data quality problems like missing values, duplicate rows, mixed data types, potential outliers, and inconsistencies.\n",
    "\n",
    "In a new Excel cell, enter `=PY` and paste the following code, then press **Ctrl+Enter**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa093d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy data with various data quality issues\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "num_records = 1000\n",
    "\n",
    "data = {\n",
    "    'ID': range(1, num_records + 1),\n",
    "    'Name': [fake.name() for _ in range(num_records)],\n",
    "    'Age': [random.randint(18, 70) for _ in range(num_records)],\n",
    "    'Salary': [round(random.uniform(30000, 120000), 2) for _ in range(num_records)],\n",
    "    'Department': [random.choice(['Sales', 'Marketing', 'IT', 'Finance', 'HR', None, 'SALES']) for _ in range(num_records)], # Includes None and case inconsistency\n",
    "    'EnrollmentDate': [fake.date_this_decade() if random.random() > 0.05 else None for _ in range(num_records)], # Missing dates\n",
    "    'Rating': [random.choice([1, 2, 3, 4, 5, None, 'N/A']) for _ in range(num_records)], # Mixed types and missing\n",
    "    'Email': [fake.email() for _ in range(num_records)],\n",
    "    'City': [fake.city() if random.random() > 0.1 else np.nan for _ in range(num_records)], # Missing cities using np.nan\n",
    "    'Comment': [fake.sentence() if random.random() > 0.2 else '' for _ in range(num_records)], # Empty strings\n",
    "\n",
    "}\n",
    "\n",
    "df_dirty = pd.DataFrame(data)\n",
    "\n",
    "# Introduce some specific issues\n",
    "# Add missing values\n",
    "for col in ['Age', 'Salary', 'Department', 'EnrollmentDate']:\n",
    "    missing_indices = random.sample(range(num_records), int(num_records * random.uniform(0.02, 0.1))) # 2-10% missing\n",
    "    df_dirty.loc[missing_indices, col] = np.nan\n",
    "\n",
    "# Add duplicate rows\n",
    "duplicate_rows = df_dirty.sample(n=int(num_records * 0.05), replace=False) # Select 5% of rows\n",
    "df_dirty = pd.concat([df_dirty, duplicate_rows], ignore_index=True)\n",
    "\n",
    "# Add potential outliers in Salary\n",
    "outlier_indices = random.sample(range(len(df_dirty)), int(len(df_dirty) * 0.01)) # 1% outliers\n",
    "df_dirty.loc[outlier_indices, 'Salary'] = df_dirty['Salary'] * random.uniform(5, 10) # Multiply salary to create high outliers\n",
    "\n",
    "# Add some negative Salary outliers (less common but possible data entry error)\n",
    "negative_outlier_indices = random.sample(range(len(df_dirty)), int(len(df_dirty) * 0.005)) # 0.5% negative outliers\n",
    "df_dirty.loc[negative_outlier_indices, 'Salary'] = df_dirty['Salary'] * -1 # Make salary negative\n",
    "\n",
    "# Add mixed data type in a numeric column\n",
    "# Introduce some strings into the Age column\n",
    "string_age_indices = random.sample(range(len(df_dirty)), int(len(df_dirty) * 0.01))\n",
    "df_dirty.loc[string_age_indices, 'Age'] = random.choice(['Twenty Five', 'Thirty', 'Forty Two', 'Unknown'])\n",
    "\n",
    "\n",
    "# Shuffle rows to mix things up\n",
    "df_dirty = df_dirty.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df_dirty # Output the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf1b585",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "*   This code generates a DataFrame `df_dirty` that simulates employee or customer records.\n",
    "*   It uses `Faker` for realistic names, emails, cities, and sentences.\n",
    "*   It intentionally injects various data quality issues:\n",
    "    *   Randomly distributed missing values (`None`, `np.nan`).\n",
    "    *   Case inconsistency in 'Department'.\n",
    "    *   Mixed data types in 'Rating' (integers, strings, None) and 'Age' (integers, strings).\n",
    "    *   Duplicate rows.\n",
    "    *   Outliers (very high and negative values) in 'Salary'.\n",
    "    *   Empty strings in 'Comment'.\n",
    "*   The result, `df_dirty`, will be spilled into your Excel sheet. Let's assume this data is placed in a range or Table named `DirtyData`.\n",
    "\n",
    "**Step 2: Perform Data Quality Assessment and Summarize Findings**\n",
    "\n",
    "Now, let's use Python to systematically identify the issues we just introduced (and any others present in your actual data).\n",
    "\n",
    "In a **new** Excel cell, enter `=PY` and paste the following code. Replace `\"DirtyData\"` with the actual name of the Excel range/Table where your dummy data is. Press **Ctrl+Enter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0547d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data quality assessment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dirty data from Excel\n",
    "# IMPORTANT: Replace \"DirtyData\" with the actual name of your Excel range or Table\n",
    "df = xl(\"DirtyData[#All]\", headers=True)\n",
    "\n",
    "# --- 1. Missing Value Analysis ---\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Missing Percentage (%)': missing_percentage\n",
    "})\n",
    "# Only show columns with missing values, sorted\n",
    "missing_df_summary = missing_df[missing_df['Missing Values'] > 0].sort_values('Missing Values', ascending=False)\n",
    "\n",
    "\n",
    "# --- 2. Duplicate Row Analysis ---\n",
    "duplicate_rows_count = df.duplicated().sum()\n",
    "\n",
    "\n",
    "# --- 3. Data Type Analysis ---\n",
    "dtype_summary = df.dtypes.reset_index()\n",
    "dtype_summary.columns = ['Column', 'DataType']\n",
    "\n",
    "\n",
    "# --- 4. Unique Value Analysis (for potential inconsistencies or cardinality) ---\n",
    "unique_value_counts = {}\n",
    "for col in df.columns:\n",
    "    # For large number of unique values, just count; for small, list top few\n",
    "    if df[col].nunique() < 50: # Arbitrary threshold\n",
    "         # Get value counts, including NaNs\n",
    "         value_counts = df[col].value_counts(dropna=False)\n",
    "         # Convert index to string for consistent output in Excel\n",
    "         value_counts.index = value_counts.index.astype(str)\n",
    "         unique_value_counts[col] = value_counts.reset_index().rename(columns={'index': 'Value', col: 'Count'})\n",
    "    else:\n",
    "        unique_value_counts[col] = pd.DataFrame({'Value': [f'{df[col].nunique()} unique values'], 'Count': [len(df)]})\n",
    "\n",
    "\n",
    "# --- 5. Outlier Detection (Example for numeric column 'Salary') ---\n",
    "# Convert 'Salary' to numeric, coercing errors to NaN\n",
    "df['Salary_numeric'] = pd.to_numeric(df['Salary'], errors='coerce')\n",
    "\n",
    "# Calculate IQR for 'Salary' (robust to outliers)\n",
    "Q1 = df['Salary_numeric'].quantile(0.25)\n",
    "Q3 = df['Salary_numeric'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers\n",
    "outliers_iqr = df[(df['Salary_numeric'] < lower_bound) | (df['Salary_numeric'] > upper_bound)]\n",
    "\n",
    "# --- 6. Mixed Data Type Detection (Example for 'Age', 'Rating') ---\n",
    "# Check for non-numeric values in 'Age'\n",
    "non_numeric_age = df[pd.to_numeric(df['Age'], errors='coerce').isna() & df['Age'].notna()] # Find non-numeric entries that are not NaN\n",
    "\n",
    "# Check for non-standard values in 'Rating' (assuming standard is 1-5 integers)\n",
    "# First, convert potential numeric strings to numeric\n",
    "df['Rating_numeric'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
    "# Find entries that are NaN in numeric conversion but not NaN in original, or outside 1-5\n",
    "non_standard_rating = df[(df['Rating_numeric'].isna() & df['Rating'].notna()) | (~df['Rating_numeric'].isna() & ~df['Rating_numeric'].isin([1, 2, 3, 4, 5]))]\n",
    "\n",
    "\n",
    "# --- Visualization for Outlier Detection ---\n",
    "\n",
    "# Apply custom style guidelines\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.edgecolor'] = '#1a1a24'\n",
    "plt.rcParams['axes.linewidth'] = 1\n",
    "plt.rcParams['axes.grid'] = False # Turn off default grid\n",
    "sns.set_theme(style=\"whitegrid\") # Use a seaborn theme base, then apply customs\n",
    "\n",
    "\n",
    "# Box plot for Salary to visualize distribution and outliers\n",
    "fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "# Use the original 'Salary' column which might have strings, let seaborn/pandas handle plotting numerics\n",
    "# Or better, use the 'Salary_numeric' column we created, dropping NaNs for plotting\n",
    "sns.boxplot(y=df['Salary_numeric'].dropna(), ax=ax1, color='#ff6d00') # Orange\n",
    "\n",
    "ax1.set_title('Distribution and Outliers of Salary', fontsize=14, color='#1a1a24')\n",
    "ax1.set_ylabel('Salary', fontsize=12, color='#1a1a24')\n",
    "# Hide x-axis label and ticks as it's a single box plot\n",
    "ax1.set_xticklabels([])\n",
    "ax1.set_xlabel('')\n",
    "sns.despine(ax=ax1, top=True, right=True, bottom=True, left=False) # Remove top, right, bottom spines\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Output results\n",
    "# Combine summaries into a dictionary for return\n",
    "output = {\n",
    "    'Missing Values Summary': missing_df_summary,\n",
    "    'Duplicate Rows Count': pd.DataFrame({'Metric': ['Total Duplicate Rows'], 'Count': [duplicate_rows_count]}), # Return as DataFrame\n",
    "    'Data Types Summary': dtype_summary,\n",
    "    'Unique Value Counts (Sample/Small N)': unique_value_counts, # Dictionary of DataFrames\n",
    "    'Salary Outliers (IQR Method)': outliers_iqr,\n",
    "    'Non-Numeric Age Entries': non_numeric_age,\n",
    "    'Non-Standard Rating Entries': non_standard_rating,\n",
    "    'Salary_Outlier_Box_Plot': fig1,\n",
    "}\n",
    "\n",
    "output # Output the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c2f2b2",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "*   We load the dummy \"dirty\" data using `xl()`. **Remember to replace `\"DirtyData\"`**.\n",
    "*   We calculate missing values per column using `isnull().sum()` and the percentage.\n",
    "*   We count duplicate rows using `duplicated().sum()`.\n",
    "*   We get a summary of detected data types using `dtypes`.\n",
    "*   We analyze unique values for each column. If there are few unique values (< 50), we list the value counts; otherwise, we just report the total unique count. This helps spot variations like \"Sales\" vs \"SALES\" or unexpected string values.\n",
    "*   We perform a basic outlier detection for the 'Salary' column using the Interquartile Range (IQR) method. We first ensure the column is numeric (`pd.to_numeric(errors='coerce')` is crucial here, turning errors into `NaN` so they don't break calculations).\n",
    "*   We check for mixed data types by trying to convert columns to numeric and identifying rows where this conversion fails but the original value wasn't missing. We also specifically look for values outside the expected range for 'Rating'.\n",
    "*   **Visualization:** A box plot is created for the `Salary_numeric` column. Box plots are excellent for visually showing the distribution, median, quartiles, and identifying potential outliers (points plotted individually beyond the \"whiskers\").\n",
    "*   **Custom Style:** Applied the specified style guidelines (font, color for the box plot, axes, spines, grid, removed bottom axis ticks/label for clarity).\n",
    "*   We return a dictionary containing various DataFrames summarizing the data quality findings (missing values, duplicates, dtypes, value counts, specific outlier/mixed type entries) and the box plot figure.\n",
    "\n",
    "**Viewing the Output:**\n",
    "\n",
    "*   Click the Python cell, then click the Python icon/button next to the formula bar.\n",
    "*   Select \"Excel Value\" (**Ctrl+Shift+Alt+M**) for the DataFrames ('Missing Values Summary', 'Duplicate Rows Count', 'Data Types Summary', 'Salary Outliers (IQR Method)', 'Non-Numeric Age Entries', 'Non-Standard Rating Entries'). Note that 'Unique Value Counts (Sample/Small N)' is a dictionary itself; you'll access its contents by referencing the cell (e.g., `=PY(A1[\"Department\"])`) and then converting that cell to Excel Value.\n",
    "*   For the plot figure object ('Salary_Outlier_Box_Plot'), select \"Picture in Cell\" > \"Create Reference\" to see the plot.\n",
    "\n",
    "This comprehensive assessment provides a clear understanding of data quality issues present in the dataset. The next topic in the series is [\"Data Cleaning & Preparation - 2. Data Transformation\"](./03-Data%20Cleaning%20&%20Preparation_02-Data%20Transformation.md), which covers techniques for addressing the types of issues identified here, including handling missing values and fixing mixed data types.\n",
    "\n",
    "**Further Analysis:**\n",
    "* **Advanced Anomaly Detection:** Using isolation forests or autoencoders to detect complex anomalies in multivariate data\n",
    "* **Data Quality Scoring:** Implementing a weighted scoring system based on completeness, accuracy, consistency, and timeliness\n",
    "* **Pattern Recognition:** Using association rule mining to identify data quality patterns and dependencies\n",
    "* **Time Series Quality:** Analyzing temporal consistency, seasonality violations, and detecting change points in time-based data\n",
    "* **Cross-Validation Analysis:** Implementing k-fold validation to assess data quality metrics' stability across different subsets"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
