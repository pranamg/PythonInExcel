{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659e6de0",
   "metadata": {},
   "source": [
    "The next topic in the predictive modeling series is **Predictive Modeling - 2. Classification (Predicting Categorical Values)**.\n",
    "\n",
    "Classification models solve the important task of predicting categorical outcomes - assigning observations to specific classes or categories. These models are essential for business applications such as predicting customer churn (Churn/No Churn), detecting fraudulent transactions (Fraud/Not Fraud), and filtering emails (Spam/Not Spam).\n",
    "\n",
    "Based on [`piplist.txt`](./README.md) output, you should have `pandas`, `numpy`, `scikit-learn` (which provides a wide array of classification algorithms, data splitting tools, and evaluation metrics), `seaborn`, and `matplotlib`. This is exactly what we need for a typical classification workflow.\n",
    "\n",
    "**Step 1: Generate Sample Data for Classification**\n",
    "\n",
    "We'll create a dummy dataset representing customer features and a binary target variable indicating whether the customer churned or not (`IsChurn`). The features will have some relationship with the likelihood of churning. We'll include missing values.\n",
    "\n",
    "In a new Excel cell, enter `=PY` and paste the following code, then press **Ctrl+Enter**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d7121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy data for Classification (Predicting Churn)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "num_records = 1500\n",
    "\n",
    "data = {\n",
    "    'CustomerID': [f'Cust{1000 + i}' for i in range(num_records)],\n",
    "    'MonthlyCharges': [round(random.uniform(20, 120), 2) for _ in range(num_records)],\n",
    "    'TotalCharges': [round(random.uniform(100, 8000), 2) for _ in range(num_records)],\n",
    "    'ContractType': [random.choice(['Month-to-month', 'One year', 'Two year']) for _ in range(num_records)],\n",
    "    'ServiceDuration_Months': [random.randint(1, 72) for _ in range(num_records)],\n",
    "    'NumServiceCalls': [random.randint(0, 10) for _ in range(num_records)],\n",
    "    'SupportScore': [random.uniform(1, 5) for _ in range(num_records)], # Higher score means better support\n",
    "    'PaymentMethod': [random.choice(['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card']) for _ in range(num_records)],\n",
    "}\n",
    "\n",
    "df_clf_data = pd.DataFrame(data)\n",
    "\n",
    "# Simulate 'IsChurn' based on some features\n",
    "# Higher MonthlyCharges, Month-to-month contract, shorter duration, more service calls, lower support score increase churn probability\n",
    "def determine_churn(row):\n",
    "    churn_prob = 0.15 # Base churn probability\n",
    "\n",
    "    if row['MonthlyCharges'] > 80:\n",
    "        churn_prob += 0.10\n",
    "    if row['ContractType'] == 'Month-to-month':\n",
    "        churn_prob += 0.25\n",
    "    if row['ServiceDuration_Months'] < 12:\n",
    "        churn_prob += 0.15\n",
    "    if row['NumServiceCalls'] > 3:\n",
    "        churn_prob += 0.10 * (row['NumServiceCalls'] - 3) # Higher calls increase prob\n",
    "    if row['SupportScore'] < 3:\n",
    "         churn_prob += 0.15 * (3 - row['SupportScore']) # Lower score increases prob\n",
    "\n",
    "    # Add some randomness\n",
    "    churn_prob += np.random.normal(0, 0.1)\n",
    "\n",
    "    # Ensure probability is between 0 and 1\n",
    "    churn_prob = max(0, min(1, churn_prob))\n",
    "\n",
    "    # Randomly assign churn based on probability\n",
    "    return random.random() < churn_prob\n",
    "\n",
    "# Apply the function to create the 'IsChurn' column (True/False)\n",
    "df_clf_data['IsChurn'] = df_clf_data.apply(determine_churn, axis=1)\n",
    "\n",
    "# Introduce some missing values\n",
    "for col in ['MonthlyCharges', 'TotalCharges', 'ServiceDuration_Months', 'SupportScore', 'NumServiceCalls']:\n",
    "    missing_indices = random.sample(range(num_records), int(num_records * random.uniform(0.02, 0.06))) # 2-6% missing\n",
    "    df_clf_data.loc[missing_indices, col] = np.nan\n",
    "\n",
    "# Introduce missing values in a categorical column\n",
    "missing_cat_indices = random.sample(range(num_records), int(num_records * 0.04)) # 4% missing\n",
    "df_clf_data.loc[missing_cat_indices, 'PaymentMethod'] = np.nan\n",
    "\n",
    "\n",
    "# Shuffle rows\n",
    "df_clf_data = df_clf_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_clf_data # Output the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54febc",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "*   This code generates a DataFrame `df_clf_data` with simulated customer data including numerical features (charges, duration, calls, support score) and categorical features (contract type, payment method), plus a binary target variable `IsChurn`.\n",
    "*   The `IsChurn` target is generated with a probability that depends on the feature values, creating a realistic (though simplified) relationship.\n",
    "*   Missing values (`np.nan`, `None`) are introduced in both numerical and categorical columns.\n",
    "*   The result, `df_clf_data`, will be spilled into your Excel sheet. Let's assume this data is placed in a range or Table named `ClassificationData`.\n",
    "\n",
    "**Step 2: Build, Evaluate, and Visualize a Classification Model**\n",
    "\n",
    "Now, we'll load this dummy data, handle missing values, encode categorical features, split the data, train a `LogisticRegression` model, make predictions, evaluate its performance using metrics and a confusion matrix, and visualize the confusion matrix.\n",
    "\n",
    "In a **new** Excel cell, enter `=PY` and paste the following code. Replace `\"ClassificationData\"` with the actual name of the Excel range/Table where your dummy data is. Press **Ctrl+Enter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd97bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build, evaluate, and visualize a Classification model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split # For splitting data\n",
    "from sklearn.linear_model import LogisticRegression # A common classification model\n",
    "from sklearn.impute import SimpleImputer # For handling missing values\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler # For encoding and scaling\n",
    "from sklearn.compose import ColumnTransformer # To apply different transformers to different columns\n",
    "from sklearn.pipeline import Pipeline # To chain transformations\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay # For evaluation and visualization\n",
    "\n",
    "\n",
    "# Load the data from Excel\n",
    "# IMPORTANT: Replace \"ClassificationData\" with the actual name of your Excel range or Table\n",
    "df = xl(\"ClassificationData[#All]\", headers=True)\n",
    "\n",
    "# Ensure target column is boolean and features are appropriate types\n",
    "# Coerce errors for numerical columns\n",
    "numerical_cols = ['MonthlyCharges', 'TotalCharges', 'ServiceDuration_Months', 'NumServiceCalls', 'SupportScore']\n",
    "categorical_cols = ['ContractType', 'PaymentMethod'] # Include payment method with NaNs\n",
    "# CustomerID is an identifier, not a feature for the model\n",
    "\n",
    "df['IsChurn'] = df['IsChurn'].astype(bool) # Ensure target is boolean\n",
    "for col in numerical_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "for col in categorical_cols:\n",
    "     # Ensure categorical columns are treated as strings for consistent imputation/encoding\n",
    "     df[col] = df[col].astype(str).replace('nan', np.nan) # Convert 'nan' string from Excel to np.nan\n",
    "\n",
    "\n",
    "# --- Data Preparation ---\n",
    "# Drop rows where the target variable is missing (if any were generated, though unlikely for binary churn)\n",
    "df_cleaned = df.dropna(subset=['IsChurn']).copy()\n",
    "\n",
    "# Separate features (X) and target (Y)\n",
    "X = df_cleaned[numerical_cols + categorical_cols]\n",
    "Y = df_cleaned['IsChurn']\n",
    "\n",
    "# Create preprocessing pipelines for different column types\n",
    "# Numeric pipeline: Impute with median, then scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline: Impute with most frequent, then one-hot encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # Use most_frequent for strings\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # sparse=False for dense array\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough' # Keep other columns if any (like CustomerID if not dropped) - though we excluded it from X\n",
    ")\n",
    "\n",
    "# Create the full pipeline: preprocessing + model\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42, solver='liblinear')) # Use liblinear for smaller datasets or L1 regularization\n",
    "])\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42, stratify=Y) # Stratify to maintain class distribution\n",
    "\n",
    "\n",
    "# --- Model Training ---\n",
    "# Train the full pipeline (preprocessing + model)\n",
    "model_pipeline.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# --- Prediction ---\n",
    "# Make predictions on the test set\n",
    "Y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "# --- Model Evaluation ---\n",
    "# Calculate common classification metrics\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "precision = precision_score(Y_test, Y_pred) # For positive class (True/Churn)\n",
    "recall = recall_score(Y_test, Y_pred)     # For positive class (True/Churn)\n",
    "f1 = f1_score(Y_test, Y_pred)           # Harmonic mean of precision and recall\n",
    "\n",
    "# Calculate Confusion Matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "# Create a DataFrame for evaluation metrics\n",
    "evaluation_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision (Churn)', 'Recall (Churn)', 'F1-Score (Churn)'],\n",
    "    'Value': [accuracy, precision, recall, f1]\n",
    "})\n",
    "\n",
    "# Create a DataFrame for the Confusion Matrix\n",
    "cm_df = pd.DataFrame(cm, index=['Actual Not Churn', 'Actual Churn'], columns=['Predicted Not Churn', 'Predicted Churn'])\n",
    "\n",
    "\n",
    "# --- Visualization ---\n",
    "\n",
    "# Apply custom style guidelines\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.edgecolor'] = '#1a1a24'\n",
    "plt.rcParams['axes.linewidth'] = 1\n",
    "plt.rcParams['axes.grid'] = False # Turn off default grid\n",
    "sns.set_theme(style=\"whitegrid\") # Use a seaborn theme base, then apply customs\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "fig1, ax1 = plt.subplots(figsize=(7, 6))\n",
    "# Use ConfusionMatrixDisplay for easier plotting\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Not Churn', 'Churn'])\n",
    "# Use a sequential color map, e.g., 'Blues' or pick from guidelines\n",
    "# Let's use a guideline color for the heatmap, e.g. blue shades or green shades\n",
    "# Blues colormap goes from light blue to dark blue\n",
    "disp.plot(cmap='Blues', ax=ax1, values_format='d') # 'd' for integer formatting\n",
    "\n",
    "ax1.set_title('Confusion Matrix', fontsize=14, color='#1a1a24')\n",
    "ax1.set_xlabel('Predicted Label', fontsize=12, color='#1a1a24')\n",
    "ax1.set_ylabel('Actual Label', fontsize=12, color='#1a1a24')\n",
    "# Customize text color within the matrix squares if needed\n",
    "for labels in ax1.texts:\n",
    "    labels.set_color(\"#1a1a24\") # Make text off-black\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Output results\n",
    "# Return a dictionary containing evaluation metrics, confusion matrix, and plot\n",
    "output = {\n",
    "    'Classification Evaluation Metrics': evaluation_metrics,\n",
    "    'Confusion Matrix': cm_df,\n",
    "    'Confusion_Matrix_Plot': fig1\n",
    "}\n",
    "\n",
    "output # Output the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f977541e",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "*   We load the dummy classification data. **Remember to replace `\"ClassificationData\"`**.\n",
    "*   We ensure the target variable `IsChurn` is treated as boolean. Numerical columns are converted using `pd.to_numeric(errors='coerce')`, and categorical columns are explicitly converted to string/object type, ensuring `NaN` is correctly represented.\n",
    "*   **Data Preparation:**\n",
    "    *   Rows with missing `IsChurn` are dropped (though our data generation didn't create these).\n",
    "    *   The data is split into features (X) and target (Y).\n",
    "    *   We use `scikit-learn` Pipelines and `ColumnTransformer` for structured preprocessing:\n",
    "        *   Numerical features are imputed with the median and then scaled using `StandardScaler`.\n",
    "        *   Categorical features are imputed with the most frequent value and then one-hot encoded using `OneHotEncoder`. `handle_unknown='ignore'` helps if new categories appear in the test set. `sparse_output=False` gives a dense array.\n",
    "    *   The `preprocessor` step is combined with the `LogisticRegression` model into a single `Pipeline`. This simplifies the workflow and helps prevent data leakage during training and prediction.\n",
    "    *   `train_test_split` divides the data (features and target) into training (75%) and testing (25%) sets. `stratify=Y` is important for classification, ensuring the proportion of the target classes (Churn/Not Churn) is similar in both the training and testing sets.\n",
    "*   **Model Training:** The `model_pipeline` (which includes both preprocessing and the classifier) is trained (`.fit()`) on the training data (`X_train`, `Y_train`). The pipeline automatically applies the preprocessing steps before training the classifier.\n",
    "*   **Prediction:** The trained pipeline is used to make predictions (`.predict()`) on the raw test set features (`X_test`). The pipeline automatically applies the same preprocessing steps used during training to `X_test` before passing it to the classifier.\n",
    "*   **Model Evaluation:**\n",
    "    *   Common classification metrics (`accuracy`, `precision`, `recall`, `f1_score`) are calculated by comparing the model's predictions (`Y_pred`) to the actual test set values (`Y_test`).\n",
    "    *   A `confusion_matrix` is calculated. This matrix shows the counts of True Positives (correctly predicted Churn), True Negatives (correctly predicted Not Churn), False Positives (incorrectly predicted Churn), and False Negatives (incorrectly predicted Not Churn).\n",
    "*   **Visualization:**\n",
    "    *   `fig1`: A heatmap of the confusion matrix is plotted using `sklearn.metrics.ConfusionMatrixDisplay`, which is the standard way to visualize classification performance in terms of correct and incorrect predictions for each class. `cmap='Blues'` uses a blue color scheme.\n",
    "*   **Custom Style:** Applied the specified style guidelines (font, colors - using a blue color map for the heatmap, off-black for text/axes and within the matrix, axes, spines, grid - though heatmap usually doesn't need grid).\n",
    "*   We return a dictionary containing DataFrames for the evaluation metrics and the confusion matrix counts, and the confusion matrix plot figure.\n",
    "\n",
    "**Viewing the Output:**\n",
    "\n",
    "*   Click the Python cell, then click the Python icon/button next to the formula bar.\n",
    "*   Select \"Excel Value\" (**Ctrl+Shift+Alt+M**) for the DataFrames ('Classification Evaluation Metrics', 'Confusion Matrix') to spill them into your sheet.\n",
    "*   For the plot figure object ('Confusion_Matrix_Plot'), select \"Picture in Cell\" > \"Create Reference\" to see the plot.\n",
    "\n",
    "**Further Analysis:**\n",
    "\n",
    "Here are some advanced analyses you could perform on this customer churn dataset:\n",
    "\n",
    "1. **Feature Importance Analysis:**\n",
    "   - Use Random Forest's feature importance scores to identify which customer attributes most strongly predict churn\n",
    "   - Implement SHAP (SHapley Additive exPlanations) values for more detailed feature impact analysis\n",
    "\n",
    "2. **Advanced Model Comparison:**\n",
    "   - Compare LogisticRegression with other classifiers (RandomForest, XGBoost, SVM)\n",
    "   - Implement k-fold cross-validation for more robust model evaluation\n",
    "   - Use ROC curves and AUC scores for model comparison\n",
    "\n",
    "3. **Cost-Sensitive Classification:**\n",
    "   - Implement class weights to account for imbalanced data\n",
    "   - Develop a custom cost matrix considering the business impact of false positives vs. false negatives\n",
    "   - Use probability calibration to optimize decision thresholds\n",
    "\n",
    "4. **Customer Segmentation with Classification:**\n",
    "   - Combine clustering with classification for segment-specific churn prediction\n",
    "   - Analyze churn patterns within different customer segments\n",
    "   - Create targeted retention strategies based on segment-specific predictors\n",
    "\n",
    "5. **Temporal Analysis:**\n",
    "   - Implement sliding window analysis to study how churn patterns change over time\n",
    "   - Create time-based features (e.g., changes in usage patterns)\n",
    "   - Develop early warning systems using sequential pattern mining\n",
    "\n",
    "This section demonstrates a complete classification analysis workflow, from data preprocessing through model evaluation. The next topic in the series is [Predictive Modeling - 3. Time Series Forecasting](./05-Predictive%20Modeling_03-Time%20Series%20Forecasting.md), which combines elements of both regression and classification to predict future values in time-ordered data."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
